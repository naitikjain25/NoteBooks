{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f29c44-7797-419b-941b-d1e9cbd5db1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark DataFrames Project Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb178db-d80c-4f51-b14f-c23c0699fa39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's get some quick practice with your new Spark DataFrame skills, you will be asked some basic questions about some stock market data, in this case Walmart Stock from the years 2012-2017. This exercise will just ask a bunch of questions, unlike the future machine learning exercises, which will be a little looser and be in the form of \"Consulting Projects\", but more on that later!\n",
    "\n",
    "For now, just answer the questions and complete the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbfd8d8-9e47-4a1d-894c-5c9615721a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Use the walmart_stock.csv file to Answer and complete the  tasks below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d657628-6171-4608-a893-89809b467036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6128ac0c-182d-446a-9682-af8459346794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession; \n",
    "spark = SparkSession.builder.appName(\"WalmartStockAnalysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecfe074-3aa6-4039-b647-0f8c5a621fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load the Walmart Stock CSV File, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6001430b-4ee3-4ecb-84bf-b8f55d2cd001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "walmart_stock_df = spark.read.csv(\"/FileStore/tables/Walmart_stock.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3e7c18-bdcb-4e93-a408-306a6ecbf57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f87199-17af-4f73-b48c-1686f45543dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
     ]
    }
   ],
   "source": [
    "walmart_stock_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae8974d-c463-409a-8efb-08e80432ccaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79520f04-e672-4dea-9efe-6a0d89c8a6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Date: date (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Volume: integer (nullable = true)\n |-- Adj Close: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "walmart_stock_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69fcb81-006d-4dc5-bb61-9f16fd4c673b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Print out the first 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45dcec4-6689-4f56-8678-306d92aafb59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+--------+---------+\n|      Date|     Open|     High|      Low|    Close|  Volume|Adj Close|\n+----------+---------+---------+---------+---------+--------+---------+\n|2012-01-03|59.970001|61.060001|59.869999|60.330002|12668800|52.619235|\n|2012-01-04|60.209999|60.349998|59.470001|59.709999| 9593300|52.078475|\n|2012-01-05|59.349998|59.619999|58.369999|59.419998|12768200|51.825539|\n|2012-01-06|59.419998|59.450001|58.869999|     59.0| 8069400| 51.45922|\n|2012-01-09|59.029999|59.549999|58.919998|    59.18| 6679300|51.616215|\n+----------+---------+---------+---------+---------+--------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "walmart_stock_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77bdca6-44d9-4054-888d-252cd0369ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Use describe() to learn about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb6ad36-b163-4a35-a0dc-1cf26643bcc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|summary|             Open|             High|              Low|            Close|           Volume|        Adj Close|\n+-------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|  count|             1258|             1258|             1258|             1258|             1258|             1258|\n|   mean|72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n| stddev| 6.76809024470826|6.768186808159218|6.744075756255496| 6.75685916373299|  4519780.8431556|6.722609449996858|\n|    min|        56.389999|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n|    max|        90.800003|        90.970001|            89.25|        90.470001|         80898100|        84.914216|\n+-------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "walmart_stock_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce16ab14-1152-4333-bc6e-dc18b6d5eacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bonus Question!\n",
    "#### There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar. [Check this link for a hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast)\n",
    "\n",
    "If you get stuck on this, don't worry, just view the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d042bf3c-c6c1-4436-a0f8-db6395dcf890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2009930341318589>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m format_number; \n",
       "\u001B[0;32m----> 2\u001B[0m walmart_stock_df\u001B[38;5;241m.\u001B[39mdescribe()\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m[format_number(walmart_stock_df\u001B[38;5;241m.\u001B[39mdescribe()\u001B[38;5;241m.\u001B[39mselect(c)\u001B[38;5;241m.\u001B[39malias(c)\u001B[38;5;241m.\u001B[39mfirst()[c], \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39malias(c) \n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m walmart_stock_df\u001B[38;5;241m.\u001B[39mcolumns \n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `1258` cannot be resolved. Did you mean one of the following? [`Close`, `High`, `Low`, `Open`, `Volume`].;\n",
       "'Project [summary#929, format_number('1258, 2) AS Open#1283, format_number('1258, 2) AS High#1623, format_number('1258, 2) AS Low#1963, format_number('1258, 2) AS Close#2303, format_number('1258, 2) AS Volume#2643, format_number('1258, 2) AS Adj Close#2983]\n",
       "+- Project [summary#929, element_at(Open#781, summary#929, None, false) AS Open#852, element_at(High#795, summary#929, None, false) AS High#853, element_at(Low#809, summary#929, None, false) AS Low#854, element_at(Close#823, summary#929, None, false) AS Close#855, element_at(Volume#837, summary#929, None, false) AS Volume#856, element_at(Adj Close#851, summary#929, None, false) AS Adj Close#857]\n",
       "   +- Project [Open#781, High#795, Low#809, Close#823, Volume#837, Adj Close#851, summary#929]\n",
       "      +- Generate explode([count,mean,stddev,min,max]), false, [summary#929]\n",
       "         +- Aggregate [map(cast(count as string), cast(count(Open#21) as string), cast(mean as string), cast(avg(Open#21) as string), cast(stddev as string), cast(stddev_samp(Open#21) as string), cast(min as string), cast(min(Open#21) as string), cast(max as string), cast(max(Open#21) as string)) AS Open#781, map(cast(count as string), cast(count(High#22) as string), cast(mean as string), cast(avg(High#22) as string), cast(stddev as string), cast(stddev_samp(High#22) as string), cast(min as string), cast(min(High#22) as string), cast(max as string), cast(max(High#22) as string)) AS High#795, map(cast(count as string), cast(count(Low#23) as string), cast(mean as string), cast(avg(Low#23) as string), cast(stddev as string), cast(stddev_samp(Low#23) as string), cast(min as string), cast(min(Low#23) as string), cast(max as string), cast(max(Low#23) as string)) AS Low#809, map(cast(count as string), cast(count(Close#24) as string), cast(mean as string), cast(avg(Close#24) as string), cast(stddev as string), cast(stddev_samp(Close#24) as string), cast(min as string), cast(min(Close#24) as string), cast(max as string), cast(max(Close#24) as string)) AS Close#823, map(cast(count as string), cast(count(Volume#25) as string), cast(mean as string), cast(avg(Volume#25) as string), cast(stddev as string), cast(stddev_samp(cast(Volume#25 as double)) as string), cast(min as string), cast(min(Volume#25) as string), cast(max as string), cast(max(Volume#25) as string)) AS Volume#837, map(cast(count as string), cast(count(Adj Close#26) as string), cast(mean as string), cast(avg(Adj Close#26) as string), cast(stddev as string), cast(stddev_samp(Adj Close#26) as string), cast(min as string), cast(min(Adj Close#26) as string), cast(max as string), cast(max(Adj Close#26) as string)) AS Adj Close#851]\n",
       "            +- Relation [Date#20,Open#21,High#22,Low#23,Close#24,Volume#25,Adj Close#26] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2009930341318589>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m format_number; \n\u001B[0;32m----> 2\u001B[0m walmart_stock_df\u001B[38;5;241m.\u001B[39mdescribe()\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m[format_number(walmart_stock_df\u001B[38;5;241m.\u001B[39mdescribe()\u001B[38;5;241m.\u001B[39mselect(c)\u001B[38;5;241m.\u001B[39malias(c)\u001B[38;5;241m.\u001B[39mfirst()[c], \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39malias(c) \n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m walmart_stock_df\u001B[38;5;241m.\u001B[39mcolumns \n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `1258` cannot be resolved. Did you mean one of the following? [`Close`, `High`, `Low`, `Open`, `Volume`].;\n'Project [summary#929, format_number('1258, 2) AS Open#1283, format_number('1258, 2) AS High#1623, format_number('1258, 2) AS Low#1963, format_number('1258, 2) AS Close#2303, format_number('1258, 2) AS Volume#2643, format_number('1258, 2) AS Adj Close#2983]\n+- Project [summary#929, element_at(Open#781, summary#929, None, false) AS Open#852, element_at(High#795, summary#929, None, false) AS High#853, element_at(Low#809, summary#929, None, false) AS Low#854, element_at(Close#823, summary#929, None, false) AS Close#855, element_at(Volume#837, summary#929, None, false) AS Volume#856, element_at(Adj Close#851, summary#929, None, false) AS Adj Close#857]\n   +- Project [Open#781, High#795, Low#809, Close#823, Volume#837, Adj Close#851, summary#929]\n      +- Generate explode([count,mean,stddev,min,max]), false, [summary#929]\n         +- Aggregate [map(cast(count as string), cast(count(Open#21) as string), cast(mean as string), cast(avg(Open#21) as string), cast(stddev as string), cast(stddev_samp(Open#21) as string), cast(min as string), cast(min(Open#21) as string), cast(max as string), cast(max(Open#21) as string)) AS Open#781, map(cast(count as string), cast(count(High#22) as string), cast(mean as string), cast(avg(High#22) as string), cast(stddev as string), cast(stddev_samp(High#22) as string), cast(min as string), cast(min(High#22) as string), cast(max as string), cast(max(High#22) as string)) AS High#795, map(cast(count as string), cast(count(Low#23) as string), cast(mean as string), cast(avg(Low#23) as string), cast(stddev as string), cast(stddev_samp(Low#23) as string), cast(min as string), cast(min(Low#23) as string), cast(max as string), cast(max(Low#23) as string)) AS Low#809, map(cast(count as string), cast(count(Close#24) as string), cast(mean as string), cast(avg(Close#24) as string), cast(stddev as string), cast(stddev_samp(Close#24) as string), cast(min as string), cast(min(Close#24) as string), cast(max as string), cast(max(Close#24) as string)) AS Close#823, map(cast(count as string), cast(count(Volume#25) as string), cast(mean as string), cast(avg(Volume#25) as string), cast(stddev as string), cast(stddev_samp(cast(Volume#25 as double)) as string), cast(min as string), cast(min(Volume#25) as string), cast(max as string), cast(max(Volume#25) as string)) AS Volume#837, map(cast(count as string), cast(count(Adj Close#26) as string), cast(mean as string), cast(avg(Adj Close#26) as string), cast(stddev as string), cast(stddev_samp(Adj Close#26) as string), cast(min as string), cast(min(Adj Close#26) as string), cast(max as string), cast(max(Adj Close#26) as string)) AS Adj Close#851]\n            +- Relation [Date#20,Open#21,High#22,Low#23,Close#24,Volume#25,Adj Close#26] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `1258` cannot be resolved. Did you mean one of the following? [`Close`, `High`, `Low`, `Open`, `Volume`].;\n'Project [summary#929, format_number('1258, 2) AS Open#1283, format_number('1258, 2) AS High#1623, format_number('1258, 2) AS Low#1963, format_number('1258, 2) AS Close#2303, format_number('1258, 2) AS Volume#2643, format_number('1258, 2) AS Adj Close#2983]\n+- Project [summary#929, element_at(Open#781, summary#929, None, false) AS Open#852, element_at(High#795, summary#929, None, false) AS High#853, element_at(Low#809, summary#929, None, false) AS Low#854, element_at(Close#823, summary#929, None, false) AS Close#855, element_at(Volume#837, summary#929, None, false) AS Volume#856, element_at(Adj Close#851, summary#929, None, false) AS Adj Close#857]\n   +- Project [Open#781, High#795, Low#809, Close#823, Volume#837, Adj Close#851, summary#929]\n      +- Generate explode([count,mean,stddev,min,max]), false, [summary#929]\n         +- Aggregate [map(cast(count as string), cast(count(Open#21) as string), cast(mean as string), cast(avg(Open#21) as string), cast(stddev as string), cast(stddev_samp(Open#21) as string), cast(min as string), cast(min(Open#21) as string), cast(max as string), cast(max(Open#21) as string)) AS Open#781, map(cast(count as string), cast(count(High#22) as string), cast(mean as string), cast(avg(High#22) as string), cast(stddev as string), cast(stddev_samp(High#22) as string), cast(min as string), cast(min(High#22) as string), cast(max as string), cast(max(High#22) as string)) AS High#795, map(cast(count as string), cast(count(Low#23) as string), cast(mean as string), cast(avg(Low#23) as string), cast(stddev as string), cast(stddev_samp(Low#23) as string), cast(min as string), cast(min(Low#23) as string), cast(max as string), cast(max(Low#23) as string)) AS Low#809, map(cast(count as string), cast(count(Close#24) as string), cast(mean as string), cast(avg(Close#24) as string), cast(stddev as string), cast(stddev_samp(Close#24) as string), cast(min as string), cast(min(Close#24) as string), cast(max as string), cast(max(Close#24) as string)) AS Close#823, map(cast(count as string), cast(count(Volume#25) as string), cast(mean as string), cast(avg(Volume#25) as string), cast(stddev as string), cast(stddev_samp(cast(Volume#25 as double)) as string), cast(min as string), cast(min(Volume#25) as string), cast(max as string), cast(max(Volume#25) as string)) AS Volume#837, map(cast(count as string), cast(count(Adj Close#26) as string), cast(mean as string), cast(avg(Adj Close#26) as string), cast(stddev as string), cast(stddev_samp(Adj Close#26) as string), cast(min as string), cast(min(Adj Close#26) as string), cast(max as string), cast(max(Adj Close#26) as string)) AS Adj Close#851]\n            +- Relation [Date#20,Open#21,High#22,Low#23,Close#24,Volume#25,Adj Close#26] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number; \n",
    "walmart_stock_df.describe().select(\"summary\", *[format_number(walmart_stock_df.describe().select(c).alias(c).first()[c], 2).alias(c) \n",
    "for c in walmart_stock_df.columns \n",
    "if c != \"Date\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd035614-8226-4b41-a83a-a166e44e290a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f586dfad-55cd-40ef-b201-e84d552cd126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b9cac1-1998-4266-8cdf-5d75dd812063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9667e605-281f-4f51-b176-3ea777b8ae4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+\n|      Date|     High|  Volume|            HV Ratio|\n+----------+---------+--------+--------------------+\n|2012-01-03|61.060001|12668800|4.819714653321546E-6|\n|2012-01-04|60.349998| 9593300|6.290848613094555E-6|\n|2012-01-05|59.619999|12768200|4.669412994783916E-6|\n|2012-01-06|59.450001| 8069400|7.367338463826307E-6|\n|2012-01-09|59.549999| 6679300|8.915604778943901E-6|\n+----------+---------+--------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "hv_df = walmart_stock_df.withColumn(\"HV Ratio\", walmart_stock_df[\"High\"] / walmart_stock_df[\"Volume\"])\n",
    "hv_df.select(\"Date\", \"High\", \"Volume\", \"HV Ratio\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65687d07-f503-475a-828a-8974265a168b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b6b878-41f5-4304-8629-fdd50e45bd95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|      Date|     High|\n+----------+---------+\n|2015-01-13|90.970001|\n+----------+---------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "walmart_stock_df.orderBy(walmart_stock_df[\"High\"].desc()).select(\"Date\", \"High\").show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b7dd08-87ac-4e85-8d6e-90f0463e3f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is the mean of the Close column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40cacd33-4118-404f-b5e2-9f1d961183e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|       avg(Close)|\n+-----------------+\n|72.38844998012726|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "walmart_stock_df.select(mean(\"Close\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc49a14f-89ce-4a62-a106-f44d556f078f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00c4210-b55d-443a-af6a-2311915eebd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|max(Volume)|\n+-----------+\n|   80898100|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "walmart_stock_df.select(max(\"Volume\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e5139b5-b4d4-4092-9b87-08ece5dec596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|min(Volume)|\n+-----------+\n|    2094900|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "walmart_stock_df.select(min(\"Volume\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff588da-9a93-428c-bab5-9f02ca6711f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### How many days was the Close lower than 60 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3caf89a0-e5c9-42ff-812b-0fa3458c6f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 81"
     ]
    }
   ],
   "source": [
    "walmart_stock_df.filter(walmart_stock_df[\"Close\"] < 60).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d7fdd8-dfc2-4b83-90fe-c62262d06d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What percentage of the time was the High greater than 80 dollars ?\n",
    "#### In other words, (Number of Days High>80)/(Total Days in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2261df3-9034-418c-a942-97d82afe6fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of days High > 80: 9.14%\n"
     ]
    }
   ],
   "source": [
    "# Total number of days\n",
    "total_days = walmart_stock_df.count()\n",
    "\n",
    "# Number of days High > 80\n",
    "high_over_80 = walmart_stock_df.filter(walmart_stock_df[\"High\"] > 80).count()\n",
    "\n",
    "# Percentage\n",
    "percentage = (high_over_80 / total_days) * 100\n",
    "print(f\"Percentage of days High > 80: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d9c99d-0ebe-4573-9397-78bd85ded08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is the Pearson correlation between High and Volume?\n",
    "#### [Hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d20c7c-d87f-47bb-8c7b-24ddaa8b7633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between High and Volume: -0.33843260617371607\n"
     ]
    }
   ],
   "source": [
    "correlation = walmart_stock_df.corr('High', 'Volume')\n",
    "print(f\"Pearson correlation between High and Volume: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59ab03a-9e4d-4976-a9a9-5efc89400f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is the max High per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1e4f12-9f0c-4ffb-89cb-2624dd599696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n|Year| Max_High|\n+----+---------+\n|2012|77.599998|\n|2013|81.370003|\n|2014|88.089996|\n|2015|90.970001|\n|2016|75.190002|\n+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, max, avg\n",
    "\n",
    "# For max High per year\n",
    "walmart_stock_df.select(year(\"Date\").alias(\"Year\"), \"High\").groupBy(\"Year\").agg(max(\"High\").alias(\"Max_High\")).orderBy(\"Year\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d874980f-4e0d-4048-948d-0e8f0cab153c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is the average Close for each Calendar Month?\n",
    "#### In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a2d41f-bf56-45e9-99a3-d49af71d58f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n|Month|        Avg_Close|\n+-----+-----------------+\n|    1|71.44801958415842|\n|    2|  71.306804443299|\n|    3|71.77794377570092|\n|    4|72.97361900952382|\n|    5|72.30971688679247|\n|    6| 72.4953774245283|\n|    7|74.43971943925233|\n|    8|73.02981855454546|\n|    9|72.18411785294116|\n|   10|71.57854545454542|\n|   11| 72.1110893069307|\n|   12|72.84792478301885|\n+-----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, max, avg\n",
    "# For average Close per calendar month\n",
    "walmart_stock_df.select(month(\"Date\").alias(\"Month\"), \"Close\").groupBy(\"Month\").agg(avg(\"Close\").alias(\"Avg_Close\")).orderBy(\"Month\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca34bff-76f3-4a28-a12d-e149d678ae94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Walmart_Stock_Assignment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}